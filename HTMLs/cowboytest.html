<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Western Template</title>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.8.3/modernizr.min.js" type="text/javascript"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<style type="text/css">
@import url('https://fonts.googleapis.com/css2?family=PT+Mono&display=swap');

h2 {
  font-family: 'PT Mono', monospace;
  font-size: 30px;
}

h3 {
  font-family: 'PT Mono', monospace;
  font-size: 20px;
}


.abstract {
    font-style: italic;
    font-size: 14px;
}

.abstractkw {
    font-style: italic;
    font-weight: bold;
    font-size: 14px;
}
  /*Internal CSS using class name*/
    .subtitles{font-style: italic;
       font-weight: bold;
         font-size: 15px;
         color: #0a0a0a;
        display: block;
        margin-left: 30px;
      }
    /*Internal CSS for tables*/ 
    table, th {
        border-bottom:1px solid black;
        text-align: left;
        margin: 30px;
      }

    /*Internal CSS for images*/ 
    img {max-width: 90%;
          height: auto;
          margin: 30px;}
    .img3 {max-width: 50%;
          display: block;
          margin: 30px; }

/*BLOCKQUOTE*/ 

@font-face {
  font-family: 'Special Elite';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/specialelite/v18/XLYgIZbkc4JPUL5CVArUVL0ntnAOTQ.ttf) format('truetype');
}

.quotes {
  font-family: 'Special Elite', cursive;
  background: #fffdf5;
  background-color: #D28C3E;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
}


blockquote {
  font-weight: 100;
  font-size: 2rem;
  max-width: 600px;
  line-height: 1.4;
  position: relative;
  margin: 0;
  padding: 0.5rem;
}
blockquote:before,
blockquote:after {
  position: absolute;
  font-size: 8rem;
  width: 4rem;
  height: 4rem;
}
blockquote:before {
  content: '“';
  left: -5rem;
  top: -2rem;
}
blockquote:after {
  content: '”';
  right: -5rem;
  bottom: 1rem;
}
cite {
  line-height: 3;
  text-align: left;
}
/*BLOCKQUOTE*/ 
@font-face {
  font-family: "The Dead Saloon";
  src: url(/week-one/library/fonts/thedeadsaloon-Regular.ttf) format("truetype"), url(/week-one/library/fonts/thedeadsaloon-Regular.woff) format("woff");
}


html {
  font-size: 62.5%;
}

body {
  font-size: 18px;
  font-size: 1.8rem;
  line-height: 24px;
  line-height: 2.4rem;
  font-family: "The Dead Saloon", sans-serif;
  background-color: #b53c4d;
}


blockquote {
    page-break-inside: avoid;
  }

 
.background-cover {
  width: 100%;
  background-image: url(https://github.com/Technaissance/Technaissance/blob/main/Articles/Understanding%20and%20creating%20art%20with%20AI:%20review%20and%20Outlook/cowboy/cleanest.png?raw=true);
  background-size: cover;
  background-repeat: no-repeat;
  color: white;
}
.background-cover .content {
  max-width: 960px;
  margin: 0 auto;
  position: relative;
  height: 290px;
  -webkit-transform: rotate(-6deg);
  -moz-transform: rotate(-6deg);
  -ms-transform: rotate(-6deg);
  -o-transform: rotate(-6deg);
  transform: rotate(-6deg);
}
@media (min-width: 48em) {
  .background-cover .content {
    padding-top: 62px;
    height: 388px;
  }
}
.background-cover .content h1 {
  position: absolute;
  top: 50%;
  left: 50%;
  margin: 0;
  text-shadow: -2px -2px 0 #211617, 2px -2px 0 #211617, -2px 2px 0 #211617, 2px 2px 0 #211617, 6px 6px 0 #b75b21, 10px 10px 0 #d28c3d, 14px 14px 0 #b53c4d, 18px 18px 0 #b75b21, 22px 22px 0 #d28c3d;
  animation: flash .33s infinite;
  -webkit-animation: flash .33s infinite;
  font-size: 2.1875em;
  line-height: 1.37143em;
  -webkit-transform: translateX(-50%) translateY(-50%);
  -moz-transform: translateX(-50%) translateY(-50%);
  -ms-transform: translateX(-50%) translateY(-50%);
  -o-transform: translateX(-50%) translateY(-50%);
  transform: translateX(-50%) translateY(-50%);
  -webkit-transition: -webkit-transform 0.5s ease;
  -moz-transition: -moz-transform 0.5s ease;
  -o-transition: -o-transform 0.5s ease;
  transition: transform 0.5s ease;
}
@media (min-width: 48em) {
  .background-cover .content h1 {
    font-size: 4.6875em;
    line-height: 1.28em;
    text-indent: 0px;
    font-size: 50px;
    -webkit-transform: translateX(-50%) translateY(-60%);
    -moz-transform: translateX(-50%) translateY(-60%);
    -ms-transform: translateX(-50%) translateY(-60%);
    -o-transform: translateX(-50%) translateY(-60%);
    transform: translateX(-50%) translateY(-60%);
}
}

#main {
  zoom: 1;
  background-color: #d28c3d;
  color: #211617;
  max-width: 960px;
  margin: 0 auto;
  font-size: 1.5em;
  line-height: 1.5em;
}


#main .inner:before, #main .inner:after {
  content: "\0020";
  display: table;
  height: 0;
  overflow: hidden;
}
#main .inner:after {
  clear: both;
}


@keyframes flash {
  0% {
    color: #b53c4d;
    text-shadow: -2px -2px 0 #211617, 2px -2px 0 #211617, -2px 2px 0 #211617, 2px 2px 0 #211617, 6px 6px 0 #b75b21, 10px 10px 0 #d28c3d, 14px 14px 0 #b53c4d, 18px 18px 0 #b75b21, 22px 22px 0 #d28c3d;
  }

  33% {
    color: #b75b21;
    text-shadow: -2px -2px 0 #211617, 2px -2px 0 #211617, -2px 2px 0 #211617, 2px 2px 0 #211617, 6px 6px 0 #d28c3d, 10px 10px 0 #b53c4d, 14px 14px 0 #b75b21, 18px 18px 0 #d28c3d, 22px 22px 0 #b53c4d;
  }

  66% {
    color: #d28c3d;
    text-shadow: -2px -2px 0 #211617, 2px -2px 0 #211617, -2px 2px 0 #211617, 2px 2px 0 #211617, 6px 6px 0 #b53c4d, 10px 10px 0 #b75b21, 14px 14px 0 #d28c3d, 18px 18px 0 #b53c4d, 22px 22px 0 #b75b21;
  }

  100% {
    color: #b53c4d;
    text-shadow: -2px -2px 0 #211617, 2px -2px 0 #211617, -2px 2px 0 #211617, 2px 2px 0 #211617, 6px 6px 0 #b75b21, 10px 10px 0 #d28c3d, 14px 14px 0 #b53c4d, 18px 18px 0 #b75b21, 22px 22px 0 #d28c3d;
  }
}

@-webkit-keyframes flash {
  0% {
    color: #b53c4d;
    text-shadow: -2px -2px 0 #211617, 2px -2px 0 #211617, -2px 2px 0 #211617, 2px 2px 0 #211617, 6px 6px 0 #b75b21, 10px 10px 0 #d28c3d, 14px 14px 0 #b53c4d, 18px 18px 0 #b75b21, 22px 22px 0 #d28c3d;
  }

  33% {
    color: #b75b21;
    text-shadow: -2px -2px 0 #211617, 2px -2px 0 #211617, -2px 2px 0 #211617, 2px 2px 0 #211617, 6px 6px 0 #d28c3d, 10px 10px 0 #b53c4d, 14px 14px 0 #b75b21, 18px 18px 0 #d28c3d, 22px 22px 0 #b53c4d;
  }

  66% {
    color: #d28c3d;
    text-shadow: -2px -2px 0 #211617, 2px -2px 0 #211617, -2px 2px 0 #211617, 2px 2px 0 #211617, 6px 6px 0 #b53c4d, 10px 10px 0 #b75b21, 14px 14px 0 #d28c3d, 18px 18px 0 #b53c4d, 22px 22px 0 #b75b21;
  }

  100% {
    color: #b53c4d;
    text-shadow: -2px -2px 0 #211617, 2px -2px 0 #211617, -2px 2px 0 #211617, 2px 2px 0 #211617, 6px 6px 0 #b75b21, 10px 10px 0 #d28c3d, 14px 14px 0 #b53c4d, 18px 18px 0 #b75b21;
  }
}

body {
  font-family: "The Dead Saloon", sans-serif;
}

.background-cover {
  width: 100%;
  background-image: url(https://github.com/Technaissance/Technaissance/blob/main/Articles/Understanding%20and%20creating%20art%20with%20AI:%20review%20and%20Outlook/cowboy/cleanest.png?raw=true);
  background-size: cover;
  background-repeat: no-repeat;
  color: white;
}

#main .inner-content ul {
  list-style: none;
  padding: 0;
  margin: 1em 0;
}
#main .inner-content ul LI {
  line-height: 1.5em;
}
#main .inner-content ul LI:before {
  content: " ";
  background-image: url(https://s3-us-west-2.amazonaws.com/s.cdpn.io/82015/10-gallon-cowboy-hat.png);
  background-position: center center;
  background-repeat: no-repeat;
  padding-right: 1.5em;
  margin-right: 0.5em;
}

@font-face {
  font-family: "The Dead Saloon";
  src: url(https://s3-us-west-2.amazonaws.com/s.cdpn.io/82015/thedeadsaloon-Regular.ttf) format("truetype"), url(https://s3-us-west-2.amazonaws.com/s.cdpn.io/82015/thedeadsaloon-Regular.woff) format("woff");
}

/*  BIBLIOGRAPHY */

@import url('https://fonts.googleapis.com/css?family=Lora:400,700');







.nero {
    font-size: 3rem;
    margin-bottom: 0px;
    font-weight: bold;
    color: #000A00;
    background-color: #ffffff;
    font-family: "Lora", serif;
}

.bibliography {
    height: 300px;
    overflow-y: scroll;
  }

ul.bibliography {
  list-style:none;
  padding-left:0;
}

ul.bibliography li {
    color: #ffffff;
    font-family: "Lora", serif;
    color: #000A00;
    background-color: #ffffff;
    list-style: none;

}



ul.bibliography LI:before {
    content: " ";
    background-image: url(https://s3-us-west-2.amazonaws.com/s.cdpn.io/82015/10-gallon-cowboy-hat.png);
    background-position: center center;
    background-repeat: no-repeat;
    padding-right: 1.5em;


}
/*  OLD IMAGES EFFECT */


@keyframes gradientSize {
  0% {
    opacity: 0.3;
  }
  10% {
    opacity: 0.4;
  }
  20% {
    opacity: 0.25;
  }
  30% {
    opacity: 0.5;
  }
  40% {
    opacity: 0.75;
  }
  50% {
    opacity: 0.5;
  }
  60% {
    opacity: 1;
  }
  70% {
    opacity: 0.75;
  }
  80% {
    opacity: 0.5;
  }
  90% {
    opacity: 0.25;
  }
  100% {
    opacity: 0.5;
  }
}
@keyframes moveLine {
  0% {
    left: 0;
    opacity: 0;
  }
  9% {
    left: 0;
    opacity: 0;
  }
  10% {
    left: 15%;
    opacity: 0.25;
  }
  15% {
    left: 20%;
    opacity: 0.1;
  }
  20% {
    left: 10%;
    opacity: 0.2;
  }
  25% {
    left: 25%;
    opacity: 0.2;
  }
  30% {
    left: 30%;
    opacity: 0.3;
  }
  40% {
    left: 20%;
    opacity: 0.2;
  }
  50% {
    left: 30%;
    opacity: 0.1;
  }
  60% {
    left: 20%;
    opacity: 0.2;
  }
  70% {
    left: 0%;
    opacity: 0.2;
  }
  80% {
    left: 10%;
    opacity: 0.2;
  }
  90% {
    left: 30%;
    opacity: 0.3;
  }
  100% {
    left: 10%;
    opacity: 0.1;
  }
}
@keyframes bgMove {
  0% {
    background-position: 0px center;
  }
  5% {
    background-position: 0 center;
  }
  10% {
    background-position: 0 -2px;
  }
  15% {
    background-position: 0 center;
  }
  30% {
    background-position: 0px center;
  }
  37% {
    background-position: 0 -2px;
  }
  40% {
    background-position: 0px center;
  }
  45% {
    background-position: 0 2px;
  }
  50% {
    background-position: 0px center;
  }
  62% {
    background-position: 0px center;
  }
  65% {
    background-position: 0 -2px;
  }
  67% {
    background-position: 0px center;
  }
  78% {
    background-position: 0px center;
  }
  80% {
    background-position: 0 2px;
  }
  85% {
    background-position: 0 center;
  }
  87% {
    background-position: 0 center;
  }
  89% {
    background-position: 0 -2px;
  }
  91% {
    background-position: 0 center;
  }
  100% {
    background-position: 0px center;
  }
}


#container {
  width: 100%;
  height: 400px;
  left: 50%;
  transform: translateX(-50%);
  overflow: hidden;
  position: relative;
  border: 1px solid #95795b;
  background-color: rgba(0, 0, 0, 0);
  display: flex;
  align-items: center; /* vertical */
  justify-content: center; /* horizontal */
  overflow: hidden;
  animation-name: bgMove;
  animation-duration: 5s;
  animation-iteration-count: infinite;



}

#over img {
  margin-left: auto;
  margin-right: auto;
  display: block;
  margin-top: 60px; 


}

#container:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background-color: #bfac97;
  mix-blend-mode: color;
  content: " ";
}
#container #effects {
  width: 105%;
  height: 105%;
  margin-top: -20px;
  margin-left: -20px;
  position: absolute;

}
#container #effects:before {
  content: " ";
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  animation-name: gradientSize;
  animation-duration: 1.5s;
  animation-iteration-count: infinite;
  -moz-box-shadow: inset 0 0 10em black;
  -webkit-box-shadow: inset 0 0 10em black;
  box-shadow: inset 0 0 10em black;
}
#container #effects:after {
  width: 1px;
  height: 100%;
  background: black;
  content: " ";
  display: block;
  position: absolute;
  animation-name: moveLine;
  animation-duration: 8s;
  animation-iteration-count: infinite;
}





</style>

</head>
<body>
<!-- partial:index.partial.html -->
<header>
  <section class="background-cover parallax-background">
    <section class="content">
      <h1>UNDERSTANDING AND CREATING ART WITH AI: REVIEW AND OUTLOOK</h1>
    </section>
  </section>
</header>
<section id="main">
  <div class="inner">
    <div class="inner-content">
      <p>My grandpappy used to make this chili over the campfire under the Texas stars. The secret ingredient was a little squeeze of scorpion venom in the pot.</p>
    
<div class="quotes"><blockquote ><p class="quotes" >The man who comes back through the door in the wall will never be quite the same as the man who went out.</p></blockquote>
</div>
    </div>
  </div>

  <div id="container">
  <div id="effects">
    <div id="over"  style="position:absolute; width:100%; height:100%">
    <img src="https://technaissance.github.io/Technaissance/Articles/Understanding%20and%20creating%20art%20with%20AI:%20review%20and%20Outlook/article1a.png" class="img-fluid" class="imagen">
</div>

  </div>



</div>

<h4 class="nero"> References </h4>
  <ul class="bibliography">
    <li>[1] A. RAMESH, M. PAVLOV, G. G., AND GRAY, S. Dall·e: Creating images from text., 2021.</li>
 
<li>[2] ABRY, P., WENDT, H., AND JAFFARD, S. When van gogh meets mandelbrot: Multifractal classification of painting’s texture. Signal Processing 93, 3 (2013), 554–572.</li>
 
<li>[3] ACHLIOPTAS, P., OVSJANIKOV, M., HAYDAROV, K., ELHOSEINY, M., AND GUIBAS, L. Artemis: Affective language for visual art. arXiv preprint arXiv:2101.07396 (2021).</li>
 
<li>[4] AGARWAL, S., KARNICK, H., PANT, N., AND PATEL, U. Genre and style based painting classification. In</li>
 
<li>2015 IEEE Winter Conference on Applications of Computer Vision (WACV) (2015), IEEE, pp. 588–594.</li>
 
<li>[5] ALAMEDA-PINEDA, X., RICCI, E., YAN, Y., AND SEBE, N. Recognizing emotions from abstract paintings using non-linear matrix completion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2016), pp. 5240–5248.</li>
 
<li>[6] AMIRSHAHI, S. A., HAYN-LEICHSENRING, G. U., DENZLER, J., AND REDIES, C. Jenaesthetics subjective dataset: analyzing paintings by subjective scores. In European Conference on Computer Vision (2014), Springer, pp. 3–19.</li>
 
<li>[7] BAR, Y., LEVY, N., AND WOLF, L. Classification of artistic styles using binarized features derived from a deep neural network. In Computer Vision - ECCV 2014 Workshops - Zurich, Switzerland, September 6-7 and 12, 2014, Proceedings, Part I (2014), pp. 71–84.</li>
 
<li>[8] BARALDI, L., CORNIA, M., GRANA, C., AND CUCCHIARA, R. Aligning text and document illustrations: towards visually explainable digital humanities. In 2018 24th International Conference on Pattern Recognition (ICPR) (2018), IEEE, pp. 1097–1102.</li>
 
<li>[9] BELL, P., AND IMPETT, L. Ikonographie und interaktion. computergestützte analyse von posen in bildern der heilsgeschichte. Das Mittelalter 24, 1 (2019), 31–53.</li>
 
<li>[10]        BIANCO, S., MAZZINI, D., NAPOLETANO, P., AND SCHETTINI, R. Multitask painting categorization by deep multibranch neural network. Expert Systems with Applications 135 (2019), 90–101.</li>
 
<li>[11]        BODEN, M. A. Creativity and art: Three roads to surprise. Oxford University Press, 2010.</li>
 
<li>[12]        BODEN, M. A., AND EDMONDS, E. A. What is generative art? Digital Creativity 20, 1-2 (2009), 21–46.</li>
 
<li>[13]        BONGINI, P., BECATTINI, F., BAGDANOV, A. D., AND DEL BIMBO, A. Visual question answering for cultural heritage. arXiv preprint arXiv:2003.09853 (2020).</li>
 
<li>[14]        BROCK, A., DONAHUE, J., AND SIMONYAN, K. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096 (2018).</li>
 
<li>[15]        CARNEIRO, G., DA SILVA, N. P., DEL BUE, A., AND COSTEIRA, J. P. Artistic image classification: An analysis on the printart database. In European conference on computer vision (2012), Springer, pp. 143–157.</li>
 
<li>[16]        CASTELLANO, G., LELLA, E., AND VESSIO, G. Visual link retrieval and knowledge discovery in painting datasets. Multimedia Tools and Applications (2020), 1–18.</li>
 
<li>[17]        CASTELLANO, G., AND VESSIO, G. Towards a tool for visual link retrieval and knowledge discovery in painting datasets. In Italian Research Conference on Digital Libraries (2020), Springer, pp. 105–110.</li>
 
<li>[18]        CETINIC, E. Iconographic image captioning for artworks, 2021.</li>
 
<li>[19]        CETINIC, E., AND GRGIC, S. Automated painter recognition based on image feature extraction. In 2013 55th International Symposium ELMAR (2013), IEEE, pp. 19–22.</li>
 
<li>[20]        CETINIC, E., AND GRGIC, S. Genre classification of paintings. In 2016 International Symposium ELMAR, 2016 (2016), IEEE, pp. 201–204.</li>
 
<li>[21]        CETINIC, E., LIPIC, T., AND GRGIC, S. Fine-tuning convolutional neural networks for fine art classification.</li>
 
<li>Expert Systems with Applications 114 (2018), 107–118.</li>
 
<li>[22]        CETINIC, E., LIPIC, T., AND GRGIC, S. A deep learning perspective on beauty, sentiment, and remembrance of art. IEEE Access 7 (2019), 73694–73710.</li>
 
<li>[23]        CETINIC, E., LIPIC, T., AND GRGIC, S. Learning the principles of art history with convolutional neural networks. Pattern Recognition Letters 129 (2020), 56–62.</li>
 
<li>[24]        CH’NG, E. Art by computing machinery: Is machine art acceptable in the artworld? ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) 15, 2s (2019), 1–17.</li>
 
<li>[25]        CHRISTIE’S. Is artificial intelligence set to become art’s next medium?, 2018.</li>
 
<li>[26]        COECKELBERGH, M. Can machines create art? Philosophy & Technology 30, 3 (2017), 285–303.</li>
 
<li>[27]        COLTON, S. The painting fool: Stories from building an automated painter. In Computers and creativity. Springer, 2012, pp. 3–38.</li>
 
<li>[28]        COLTON, S., PEASE, A., AND SAUNDERS, R. Issues of authenticity in autonomously creative systems. In</li>
 
<li>Proceedings of the Ninth International Conference on Computational Creativity (2018).</li>
 
<li>[29]        CROWLEY, E. J., PARKHI, O. M., AND ZISSERMAN, A. Face painting: querying art with photos.</li>
 
<li>[30]        CROWLEY, E. J., AND ZISSERMAN, A. In search of art. In European Conference on Computer Vision (2014), Springer, pp. 54–70.</li>
 
<li>[31]        CROWLEY, E. J., AND ZISSERMAN, A. The state of the art: Object retrieval in paintings using discriminative regions.</li>
 
<li>[32]        CROWLEY, E. J., AND ZISSERMAN, A. The art of detection. In European Conference on Computer Vision</li>
 
<li>(2016), Springer, pp. 721–737.</li>
 
<li>[33]        DANIELE, A., AND SONG, Y.-Z. Ai+ art= human. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (2019), pp. 155–161.</li>
 
<li>[34]        DAVID, O. E., AND NETANYAHU, N. S. Deeppainter: Painter classification using deep convolutional autoen- coders. In Artificial Neural Networks and Machine Learning - ICANN 2016 - 25th International Conference on Artificial Neural Networks, Barcelona, Spain, September 6-9, 2016, Proceedings, Part II (2016), Springer, pp. 20–28.</li>
 
<li>[35]        DENG, J., DONG, W., SOCHER, R., LI, L.-J., LI, K., AND FEI-FEI, L. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on (2009), IEEE, pp. 248–255.</li>
 
<li>[36]        DENG, Y., TANG, F., DONG, W., MA, C., HUANG, F., DEUSSEN, O., AND XU, C. Exploring the representa- tivity of art paintings. IEEE Transactions on Multimedia (2020).</li>
 
<li>[37]        DORIN, A. Chance and complexity: stochastic and generative processes in art and creativity. In Proceedings of the Virtual Reality International Conference: Laval Virtual (2013), pp. 1–8.</li>
 
<li>[38]        DORIN, A., MCCABE, J., MCCORMACK, J., MONRO, G., AND WHITELAW, M. A framework for understand- ing generative art. Digital Creativity 23, 3-4 (2012), 239–259.</li>
 
<li>[39]        EFROS, A. A., AND FREEMAN, W. T. Image quilting for texture synthesis and transfer. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques (2001), pp. 341–346.</li>
 
<li>[40]        ELGAMMAL, A. Ai is blurring the definition of artist: Advanced algorithms are using machine learning to create art autonomously. American Scientist 107, 1 (2019), 18–22.</li>
 
<li>[41]        ELGAMMAL, A., LIU, B., ELHOSEINY, M., AND MAZZONE, M. Can: Creative adversarial networks, generating" art" by learning about styles and deviating from style norms. arXiv preprint arXiv:1706.07068 (2017).</li>
 
<li>[42]        ELGAMMAL, A., LIU, B., KIM, D., ELHOSEINY, M., AND MAZZONE, M. The shape of art history in the eyes of the machine. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 (2018), AAAI press, pp. 2183–2191.</li>
 
<li>[43]        EPSTEIN, Z., LEVINE, S., RAND, D. G., AND RAHWAN, I. Who gets credit for ai-generated art?  Iscience 23, 9 (2020), 101515.</li>
 
<li>[44]        ESHRAGHIAN, J. K. Human ownership of artificial creativity. Nature Machine Intelligence 2, 3 (2020), 157–160.</li>
 
<li>[45]        FLOREA, C., CONDOROVICI, R., VERTAN, C., BUTNARU, R., FLOREA, L., AND VR NCEANU, R. Pandora: Description of a painting database for art movement recognition with baselines and perspectives. In 2016 24th European Signal Processing Conference (EUSIPCO) (2016), IEEE, pp. 918–922.</li>
 
<li>[46]        FRANCESCHET, M., COLAVIZZA, G., SMITH, T., FINUCANE, B., OSTACHOWSKI, M. L., SCALET, S., PERKINS, J., MORGAN, J., AND HERNÁNDEZ, S. Crypto art: A decentralized view. Leonardo (2020), 1–8.</li>
 
<li>[47]        GALANTER, P. What is generative art? complexity theory as a context for art theory. In In GA2003–6th Generative Art Conference (2003), Citeseer.</li>
 
<li>[48]        GARCIA, N., AND VOGIATZIS, G. How to read paintings: semantic art understanding with multi-modal retrieval. In Computer Vision - ECCV 2018 Workshops - Munich, Germany, September 8-14, 2018, Proceedings, Part II (2018), vol. 11130 of Lecture Notes in Computer Science, Springer, pp. 676–691.</li>
 
<li>[49]        GARCIA, N., YE, C., LIU, Z., HU, Q., OTANI, M., CHU, C., NAKASHIMA, Y., AND MITAMURA, T. A dataset and baselines for visual question answering on art. In European Conference on Computer Vision (2020), Springer, pp. 92–108.</li>
 
<li>[50]        GATYS, L. A., ECKER, A. S., AND BETHGE, M. Image style transfer using convolutional neural networks. In</li>
 
<li>Proceedings of the IEEE conference on computer vision and pattern recognition (2016), pp. 2414–2423.</li>
 
<li>[51]        GILLOTTE, J. L. Copyright infringement in ai-generated artworks. UC Davis L. Rev. 53 (2019), 2655.</li>
 
<li>[52]        GIRSHICK, R., DONAHUE, J., DARRELL, T., AND MALIK, J. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (2014), pp. 580–587.</li>
 
<li>[53]        GONTHIER, N., GOUSSEAU, Y., AND LADJAL, S. An analysis of the transfer learning of convolutional neural networks for artistic images. arXiv preprint arXiv:2011.02727 (2020).</li>
 
<li>[54]        GONTHIER, N., GOUSSEAU, Y., LADJAL, S., AND BONFAIT, O. Weakly supervised object detection in artworks. In Proceedings of the European Conference on Computer Vision (ECCV) (2018), pp. 0–0.</li>
 
<li>[55]        GOOCH, B., AND GOOCH, A. Non-photorealistic rendering. CRC Press, 2001.</li>
 
<li>[56]        GOODFELLOW, I., POUGET-ABADIE, J., MIRZA, M., XU, B., WARDE-FARLEY, D., OZAIR, S., COURVILLE, A., AND BENGIO, Y. Generative adversarial nets. Advances in neural information processing systems 27 (2014), 2672–2680.</li>
 
<li>[57]        GRAHAM, D. J., HUGHES, J. M., LEDER, H., AND ROCKMORE, D. N. Statistics, vision, and the analysis of artistic style. Wiley Interdisciplinary Reviews: Computational Statistics 4, 2 (2012), 115–123.</li>
 
<li>[58]        GUADAMUZ, A. Do androids dream of electric copyright? comparative analysis of originality in artificial intelligence generated works. Intellectual property quarterly (2017).</li>
 
<li>[59]        HAYN-LEICHSENRING, G. U., LEHMANN, T., AND REDIES, C. Subjective ratings of beauty and aesthetics: cor- relations with statistical image properties in western oil paintings. i-Perception 8, 3 (2017), 2041669517715474.</li>
 
<li>[60]        HERTZMANN, A. Painterly rendering with curved brush strokes of multiple sizes. In Proceedings of the 25th annual conference on Computer graphics and interactive techniques (1998), pp. 453–460.</li>
 
<li>[61]        HERTZMANN, A.  Can computers create art? In Arts (2018), vol. 7, Multidisciplinary Digital Publishing Institute, p. 18.</li>
 
<li>[62]        HERTZMANN, A. Computers do not make art, people do. Communications of the ACM 63, 5 (2020), 45–48.</li>
 
<li>[63]        HERTZMANN, A. Visual indeterminacy in gan art. Leonardo 53, 4 (2020), 424–428.</li>
 
<li>[64]        HERTZMANN, A., JACOBS, C. E., OLIVER, N., CURLESS, B., AND SALESIN, D. H. Image analogies. In</li>
 
<li>Proceedings of the 28th annual conference on Computer graphics and interactive techniques (2001), pp. 327–340.</li>
 
<li>[65]        HONG, J.-W. Bias in perception of art produced by artificial intelligence. In International Conference on Human-Computer Interaction (2018), Springer, pp. 290–303.</li>
 
<li>[66]        HONG, J.-W., AND CURRAN, N. M. Artificial intelligence, artists, and art: attitudes toward artwork produced by humans vs. artificial intelligence. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) 15, 2s (2019), 1–16.</li>
 
<li>[67]        JACOBSEN, C. R., AND NIELSEN, M. Stylometry of paintings using hidden markov modelling of contourlet transforms. Signal Processing 93, 3 (2013), 579–591.</li>
 
<li>[68]        JENICEK, T., AND CHUM, O. Linking art through human poses. In 2019 International Conference on Document Analysis and Recognition (ICDAR) (2019), IEEE, pp. 1338–1345.</li>
 
<li>[69]        JING, Y., YANG, Y., FENG, Z., YE, J., YU, Y., AND SONG, M. Neural style transfer: A review. IEEE transactions on visualization and computer graphics (2019).</li>
 
<li>[70]        KARAYEV, S., TRENTACOSTE, M., HAN, H., AGARWALA, A., DARRELL, T., HERTZMANN, A., AND WINNEMOELLER, H. Recognizing image style. In British Machine Vision Conference, BMVC 2014, Nottingham, UK, September 1-5, 2014 (2014), BMVA Press.</li>
 
<li>[71]        KARRAS, T., LAINE, S., AND AILA, T. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (2019), pp. 4401–4410.</li>
 
<li>[72]        KEREN, D. Painter identification using local features and naive bayes. In 16th International Conference on Pattern Recognition, ICPR 2002, Quebec, Canada, August 11-15, 2002. (2002), vol. 2, pp. 474–477.</li>
 
<li>[73]        KHALILI, A., AND BOUCHACHIA, H. An information theory approach to aesthetic assessment of visual patterns.</li>
 
<li>Entropy 23, 2 (2021), 153.</li>
 
<li>[74]        KHAN, F. S., BEIGPOUR, S., VAN DE WEIJER, J., AND FELSBERG, M. Painting-91: a large scale database for computational painting categorization. Machine vision and applications 25, 6 (2014), 1385–1397.</li>
 
<li>[75]        KIM, D., LIU, B., ELGAMMAL, A., AND MAZZONE, M. Finding principal semantics of style in art. In 2018 IEEE 12th International Conference on Semantic Computing (ICSC), IEEE, pp. 156–163.</li>
 
<li>[76]        KIM, D., SON, S.-W., AND JEONG, H. Large-scale quantitative analysis of painting arts. Scientific reports 4</li>
 
<li>(2014), 7370.</li>
 
<li>[77]        KIM, D., XU, J., ELGAMMAL, A., AND MAZZONE, M. Computational analysis of content in fine art paintings. In ICCC (2019), pp. 33–40.</li>
 
<li>[78]        KLINKE, H. The digital transformation of art history. In The Routledge Companion to Digital Humanities and Art History. Routledge, 2020, pp. 32–42.</li>
 
<li>[79]        LANG, S., AND OMMER, B. Attesting similarity: Supporting the organization and study of art image collections with computer vision. Digital Scholarship in the Humanities 33, 4 (2018), 845–856.</li>
 
<li>[80]        LANG, S., AND OMMER, B. Reflecting on how artworks are processed and analyzed by computer vision: Supplementary material.   In Proceedings of the European Conference on Computer Vision (ECCV) (2018), pp. 0–0.</li>
 
<li>[81]        LECOUTRE, A., NÉGREVERGNE, B., AND YGER, F. Recognizing art style automatically in painting with deep learning. In Proceedings of The 9th Asian Conference on Machine Learning, ACML 2017, Seoul, Korea, November 15-17, 2017. (2017), pp. 327–342.</li>
 
<li>[82]        LEE, B., SEO, M. K., KIM, D., SHIN, I.-S., SCHICH, M., JEONG, H., AND HAN, S. K. Dissecting landscape art history with information theory. Proceedings of the National Academy of Sciences 117, 43 (2020), 26580–26590.</li>
 
<li>[83]        LIN, H., VAN ZUIJLEN, M., WIJNTJES, M. W., PONT, S. C., AND BALA, K. Insights from a large-scale database of material depictions in paintings. arXiv preprint arXiv:2011.12276 (2020).</li>
 
<li>[84]        LLANO, M. T., D’INVERNO, M., YEE-KING, M., MCCORMACK, J., ILSAR, A., PEASE, A., AND COLTON,</li>
 
<li>S. Explainable computational creativity. In Proc. ICCC (2020).</li>
 
<li>[85]        MADHU, P., KOSTI, R., MÜHRENBERG, L., BELL, P., MAIER, A., AND CHRISTLEIN, V. Recognizing charac- ters in art history using deep learning. In Proceedings of the 1st Workshop on Structuring and Understanding of Multimedia heritAge Contents (2019), pp. 15–22.</li>
 
<li>[86]        MADHU, P., MARQUART, T., KOSTI, R., BELL, P., MAIER, A., AND CHRISTLEIN, V. Understanding compositional structures in art historical images using pose and gaze priors. In European Conference on Computer Vision (2020), Springer, pp. 109–125.</li>
 
<li>[87]        MAO, H., CHEUNG, M., AND SHE, J. Deepart: Learning joint representations of visual arts. In Proceedings of the 25th ACM international conference on Multimedia (2017), pp. 1183–1191.</li>
 
<li>[88]        MAZZONE, M., AND ELGAMMAL, A. Art, creativity, and the potential of artificial intelligence. In Arts (2019), vol. 8, Multidisciplinary Digital Publishing Institute, p. 26.</li>
 
<li>[89]        MCCORMACK, J., BOWN, O., DORIN, A., MCCABE, J., MONRO, G., AND WHITELAW, M. Ten questions concerning generative computer art. Leonardo 47, 2 (2014), 135–141.</li>
 
<li>[90]        MCCORMACK, J., GIFFORD, T., AND HUTCHINGS, P. Autonomy, authenticity, authorship and intention in computer generated art. In International Conference on Computational Intelligence in Music, Sound, Art and Design (Part of EvoStar) (2019), Springer, pp. 35–50.</li>
 
<li>[91]        MENIS-MASTROMICHALAKIS, O., SOFOU, N., AND STAMOU, G. Deep ensemble art style recognition. In</li>
 
<li>2020 International Joint Conference on Neural Networks (IJCNN) (2020), IEEE, pp. 1–8.</li>
 
<li>[92]        MENSINK, T., AND VAN GEMERT, J. The rijksmuseum challenge: Museum-centered visual recognition.  In</li>
 
<li>Proceedings of International Conference on Multimedia Retrieval (2014), pp. 451–454.</li>
 
<li>[93]        MERMET, A., KITAMOTO, A., SUZUKI, C., AND TAKAGISHI, A. Face detection on pre-modern japanese artworks using r-cnn and image patching for semi-automatic annotation. In Proceedings of the 2nd Workshop on Structuring and Understanding of Multimedia heritAge Contents (2020), pp. 23–31.</li>
 
<li>[94]        MILANI, F., AND FRATERNALI, P. A data set and a convolutional model for iconography classification in paintings. arXiv preprint arXiv:2010.11697 (2020).</li>
 
<li>[95]        MOHAMMAD, S., AND KIRITCHENKO, S. Wikiart emotions: An annotated dataset of emotions evoked by art. In Proceedings of the eleventh international conference on language resources and evaluation (LREC 2018) (2018).</li>
 
<li>[96]        MORDVINTSEV, A., OLAH, C., AND TYKA, M. Inceptionism: Going deeper into neural networks, june 2015.</li>
 
<li>URL http://googleresearch. blogspot. com/2015/06/inceptionism-going-deeper-into-neural. html (2015).</li>
 
<li>[97]        MZOUGHI, O., BIGAND, A., AND RENAUD, C. Face detection in painting using deep convolutional neural networks. In International Conference on Advanced Concepts for Intelligent Vision Systems (2018), Springer, pp. 333–341.</li>
 
<li>[98]        NOTARO, A. State-of-the-art: Ai through the (artificial) artist’s eye. In EVA London 2020: Electronic Visualisation and the Arts (2020), pp. 322–328.</li>
 
<li>[99]        POSTHUMUS, E. Brill iconclass ai test set, 2020.</li>
 
<li>[100]       QI, H., AND HUGHES, S. A new method for visual stylometry on impressionist paintings. In 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2011), IEEE, pp. 2036–2039.</li>
 
<li>[101]       RAGOT, M., MARTIN, N., AND COJEAN, S. Ai-generated vs. human artworks. a perception bias towards artificial intelligence? In Extended abstracts of the 2020 CHI conference on human factors in computing systems (2020), pp. 1–10.</li>
 
<li>[102]       REA, N. Has artificial intelligence brought us the next great art movement? here are 9 pioneering artists who are exploring ai’s creative potential., 2018.</li>
 
<li>[103]       REED, S., AKATA, Z., YAN, X., LOGESWARAN, L., SCHIELE, B., AND LEE, H. Generative adversarial text to image synthesis. In International Conference on Machine Learning (2016), PMLR, pp. 1060–1069.</li>
 
<li>[104]       RISSET, J. Stochastic processes in music and art. In Stochastic Processes in Quantum Theory and Statistical Physics. Springer, 1982, pp. 281–288.</li>
 
<li>[105]       SABATELLI, M., KESTEMONT, M., DAELEMANS, W., AND GEURTS, P. Deep transfer learning for art classification problems. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops (2018), pp. 0–0.</li>
 
<li>[106]       SANDOVAL, C., PIROGOVA, E., AND LECH, M. Two-stage deep learning approach to the classification of fine-art paintings. IEEE Access 7 (2019), 41770–41781.</li>
 
<li>[107]       SARGENTIS, G., DIMITRIADIS, P., KOUTSOYIANNIS, D., ET AL. Aesthetical issues of leonardo da vinci’s and pablo picasso’s paintings with stochastic evaluation. Heritage 3, 2 (2020), 283–305.</li>
 
<li>[108]       SEGUIN, B., STRIOLO, C., KAPLAN, F., ET AL. Visual link retrieval in a database of paintings. In European Conference on Computer Vision (2016), Springer, pp. 753–767.</li>
 
<li>[109]       SHAMIR, L., MACURA, T., ORLOV, N., ECKLEY, D. M., AND GOLDBERG, I. G. Impressionism, expression- ism, surrealism: Automated recognition of painters and schools of art. ACM Transactions on Applied Perception (TAP) 7, 2 (2010), 8.</li>
 
<li>[110]       SHAMIR, L., AND TARAKHOVSKY, J. A. Computer analysis of art. J. Comput. Cult. Herit. (JOCCH) 5, 2 (Aug. 2012), 7:1–7:11.</li>
 
<li>[111]       SHEN, X., EFROS, A. A., AND AUBRY, M. Discovering visual patterns in art collections with spatially- consistent feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2019), pp. 9278–9287.</li>
 
<li>[112]       SHENG, S., AND MOENS, M.-F. Generating captions for images of ancient artworks. In Proceedings of the 27th ACM International Conference on Multimedia (2019), pp. 2478–2486.</li>
 
<li>[113]       SIDOROVA, E. The cyber turn of the contemporary art market. In Arts (2019), vol. 8, Multidisciplinary Digital Publishing Institute, p. 84.</li>
 
<li>[114]       SIGAKI, H. Y., PERC, M., AND RIBEIRO, H. V. History of art paintings through the lens of entropy and complexity. Proceedings of the National Academy of Sciences 115, 37 (2018), E8585–E8594.</li>
 
<li>[115]       SRINIVASAN, R., AND UCHINO, K. Biases in generative art—a causal look from the lens of art history. arXiv preprint arXiv:2010.13266 (2020).</li>
 
<li>[116]       STEFANINI, M., CORNIA, M., BARALDI, L., CORSINI, M., AND CUCCHIARA, R. Artpedia: A new visual- semantic dataset with visual and contextual sentences in the artistic domain. In International Conference on Image Analysis and Processing (2019), Springer, pp. 729–740.</li>
 
<li>[117]       STEPHENSEN, J. L. Towards a philosophy of post-creative practices?–reading obvious’ "portrait of edmond de belamy". Politics of the Machine Beirut 2019 2 (2019), 21–30.</li>
 
<li>[118]       STREZOSKI, G., AND WORRING, M. Omniart: a large-scale artistic benchmark. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) 14, 4 (2018), 1–21.</li>
 
<li>[119]       TODOROV,   P. A game of dice:   Machine learning and the question concerning art.     arXiv preprint arXiv:1904.01957 (2019).</li>
 
<li>[120]       VAN NOORD, N., AND POSTMA, E. Learning scale-variant and scale-invariant features for deep image classification. Pattern Recognition 61 (2017), 583–592.</li>
 
<li>[121]       VASWANI, A., SHAZEER, N., PARMAR, N., USZKOREIT, J., JONES, L., GOMEZ, A. N., KAISER, L., AND</li>
 
<li>POLOSUKHIN, I. Attention is all you need. arXiv preprint arXiv:1706.03762 (2017).</li>
 
<li>[122]       WECHSLER, H., AND TOOR, A. S. Modern art challenges face detection. Pattern Recognition Letters 126</li>
 
<li>(2019), 3–10.</li>
 
<li>[123]       WESTLAKE, N., CAI, H., AND HALL, P. Detecting people in artwork with cnns. In European Conference on Computer Vision (2016), Springer, pp. 825–841.</li>
 
<li>[124]       WU, Y., MOU, Y., LI, Z., AND XU, K. Investigating american and chinese subjects’ explicit and implicit perceptions of ai-generated artistic work. Computers in Human Behavior 104 (2020), 106186.</li>
 
<li>[125]       XU, T., ZHANG, P., HUANG, Q., ZHANG, H., GAN, Z., HUANG, X., AND HE, X. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (2018), pp. 1316–1324.</li>
 
<li>[126]       YANG, H., AND MIN, K. Classification of basic artistic media based on a deep convolutional approach. The Visual Computer 36, 3 (2020), 559–578.</li>
 
<li>[127]       YANISKY-RAVID, S., AND VELEZ-HERNANDEZ,  L. A. Copyrightability of artworks produced by creative robots and originality: the formality-objective model. Minn. JL Sci. & Tech. 19 (2018), 1.</li>
 
<li>[128]       YANULEVSKAYA, V., UIJLINGS, J., BRUNI, E., SARTORI, A., ZAMBONI, E., BACCI, F., MELCHER, D., AND SEBE, N. In the eye of the beholder: employing statistical analysis and eye tracking for analyzing abstract paintings. In Proceedings of the 20th ACM international conference on multimedia (2012), pp. 349–358.</li>
 
<li>[129]       ZHAO, L., SHANG, M., GAO, F., LI, R., HUANG, F., AND YU, J. Representation learning of image composition for aesthetic prediction. Computer Vision and Image Understanding 199 (2020), 103024.</li>
 
<li>[130]       ZHU, J.-Y., PARK, T., ISOLA, P., AND EFROS, A. A. Unpaired image-to-image translation using cycle- consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision (2017), pp. 2223–2232.</li>
 
  </ul>
</div>
</section>



<!-- partial -->
  <script src='//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js'></script><script  src="./script.js"></script>

</body>
</html>
