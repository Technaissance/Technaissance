<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <link href='https://fonts.googleapis.com/css?family=VT323' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
  <title>Sito in costruzion</title>
  <meta name="description" content="">
  <meta name="author" content="">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=PT+Mono&display=swap" rel="stylesheet">
  <script type="text/javascript" src="JS/testJSCyberpunk.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">

<style type="text/css">
 /* INNER TEXT */
 .innertext {
   font-family: "VT323";
 }


/* BODY */

:root {
  --glitched-duration: 0.9s;
  --glitched-long-duration: 3s;
  --yellow-color: #f9f002;
  --yellow-color-opacity: #f9f00242;
  --orange-color: #ff9800;
  --border-color: #8ae66e;
  --red-color: #ff003c;
  --blue-color: #136377;
  --green-color: #446d44;
  --purple-color: purple;
}

body {
  background-color: var(--yellow-color);
  min-height: 220vh;
  margin: 30px;
}

p {
  font-family:'PT Mono', monospace;

}


/* Titles */

h1.cyberpunk,
h2.cyberpunk,
h3.cyberpunk,
h4.cyberpunk {
  font-size: 2rem;
  line-height: 2.2rem;
  font-weight: 200;
  position: relative;
  padding-bottom: 15px;
 font-family: "Advent Pro", arial;

}

h2.cyberpunk {
  font-size: 1.7rem;
  line-height: 1.9rem;
  font-weight: 300;
}

h3.cyberpunk {
  font-size: 1.4rem;
  line-height: 1.6rem;
  font-weight: 500;
}

h4.cyberpunk {
  font-size: 1rem;
  line-height: 1.2rem;
  font-weight: 700;
}

h1.cyberpunk:before,
h2.cyberpunk:before,
h3.cyberpunk:before,
h4.cyberpunk:before {
    content: "";
    display: block;
    position: absolute;
    bottom: 0px;
    left: 2px;
    width: 100%;
    height: 10px;
    background-color: #000;
    clip-path: polygon(0px 0px, 85px 0px, 90px 5px, 100% 5px, 100% 6px, 85px 6px, 80px 10px, 0px 10px);
}

h1.cyberpunk.glitched {
  animation-name: h1glitched;
  animation-duration: calc(var(--glitched-duration) * 1.4);
  animation-iteration-count: infinite;
  animation-timing-function: linear;
}

h2.cyberpunk.glitched {
  animation-name: h1glitched;
  animation-duration: calc(var(--glitched-duration) * 1.7);
  animation-iteration-count: infinite;
  animation-direction: reverse;
  animation-timing-function: linear;
}

h3.cyberpunk.glitched {
  animation-name: h1glitched;
  animation-duration: calc(var(--glitched-duration) * 1.1);
  animation-iteration-count: infinite;
  animation-direction: reverse;
  animation-timing-function: ease-out;
}

h4.cyberpunk.glitched {
  animation-name: h1glitched;
  animation-duration: calc(var(--glitched-duration) * 2.1);
  animation-iteration-count: infinite;
  animation-timing-function: ease-in-out;
}

@keyframes h1glitched {
  0% {
    transform: skew(-20deg);
    left: -4px;
  }
  10% {
    transform: skew(-20deg);
    left: -4px;
  }
  11% {
    transform: skew(0deg);
    left: 2px;
  }
  50% {
    transform: skew(0deg);
  }
  51% {
    transform: skew(10deg);
  }
  59% {
    transform: skew(10deg);
  }
  60% {
    transform: skew(0deg);
  }
  100% {
    transform: skew(0deg);
  }
}

h1.cyberpunk.glitched:before {
  animation-name: h1beforeglitched;
  animation-duration: calc(var(--glitched-duration) * 2);
  animation-iteration-count: infinite;
  animation-timing-function: linear;
}

@keyframes h1beforeglitched {
  0% {
    transform: skew(-20deg);
    left: -4px;
    clip-path: polygon(0px 0px, 85px 0px, 90px 5px, 100% 5px, 100% 6px, 85px 6px, 80px 10px, 0px 10px);
  }
  10% {
    transform: skew(-20deg);
    left: -4px;
    clip-path: polygon(0px 0px, 85px 0px, 90px 5px, 100% 5px, 100% 6px, 85px 6px, 80px 10px, 0px 10px);
  }
  11% {
    transform: skew(0deg);
    left: 2px;
    clip-path: polygon(0px 0px, 85px 0px, 90px 5px, 100% 5px, 100% 6px, 85px 6px, 80px 10px, 0px 10px);
  }
  50% {
    transform: skew(0deg);
    clip-path: polygon(0px 0px, 85px 0px, 90px 5px, 100% 5px, 100% 6px, 85px 6px, 80px 10px, 0px 10px);
  }
  51% {
    transform: skew(0deg);
    clip-path: polygon(0px 0px, 85px 0px, 90px 5px, 100% 5px, 40% 5px, calc(40% - 30px) 0px, calc(40% + 30px) 0px, calc(45% - 15px) 5px, 100% 5px, 100% 6px, calc(45% - 14px) 6px, calc(40% + 29px) 1px, calc(40% - 29px) 1px, calc(40% + 1px) 6px, 85px 6px, 80px 10px, 0px 10px);
  }
  59% {
    transform: skew(0deg);
    clip-path: polygon(0px 0px, 85px 0px, 90px 5px, 100% 5px, 40% 5px, calc(40% - 30px) 0px, calc(40% + 30px) 0px, calc(45% - 15px) 5px, 100% 5px, 100% 6px, calc(45% - 14px) 6px, calc(40% + 29px) 1px, calc(40% - 29px) 1px, calc(40% + 1px) 6px, 85px 6px, 80px 10px, 0px 10px);
  }
  60% {
    transform: skew(0deg);
    clip-path: polygon(0px 0px, 85px 0px, 90px 5px, 100% 5px, 100% 6px, 85px 6px, 80px 10px, 0px 10px);
  }
  100% {
    transform: skew(0deg);
    clip-path: polygon(0px 0px, 85px 0px, 90px 5px, 100% 5px, 100% 6px, 85px 6px, 80px 10px, 0px 10px);
  }
}

h2.cyberpunk:before {
  clip-path: polygon(0px 5px, 35px 5px, 40px 0px, 85px 0px, 90px 5px, 100% 5px, 100% 6px, 85px 6px, 80px 10px, 20px 10px, 15px 6px, 0px 6px);
}

h2.cyberpunk.glitched:before {
  animation-name: h2beforeglitched;
  animation-duration: calc(var(--glitched-duration) * 2);
  animation-iteration-count: infinite;
  animation-timing-function: linear;
}

@keyframes h2beforeglitched {
  0% {
    transform: scaleY(1);
  }
  10% {
    transform: scaleY(1);
  }
  11% {
    transform: scaleY(-1);
  }
  50% {
    transform: scaleY(-1);
  }
  51% {
    transform: scaleY(1);
  }
  59% {
    transform: scaleY(1);
  }
  60% {
    transform: scaleY(1);
  }
  100% {
    transform: scaleY(1);
  }
}

h3.cyberpunk:before {
        clip-path: polygon(0px 5px, 10px 5px, 15px 0px, 40px 0px, 45px 5px, 100% 5px, 100% 6px, 31px 6px, 27px 2px, 15px 2px, 8px 10px, 0px 10px);
}

h4.cyberpunk:before {
        clip-path: polygon(0px 3px, 15px 3px, 20px 0px, 80px 0px, 85px 3px, 100% 3px, 100% 4px, 85px 4px, 80px 7px, 20px 7px, 15px 4px, 0px 4px);
}

h1.cyberpunk:after,
h2.cyberpunk:after,
h3.cyberpunk:after,
h4.cyberpunk:after,
p.cyberpunk:after {
  content: "_";
  animation-name: hxafter;
  animation-duration: var(--glitched-duration);
  animation-iteration-count: infinite;
  animation-timing-function: linear;
}

h3.cyberpunk:after,
h4.cyberpunk:after {
  animation-direction: reverse;
  animation-duration: calc(var(--glitched-duration) / 2);
}

@keyframes hxafter {
  0% {
    opacity: 0;
  }
  50% {
    opacity: 0;
  }
  51% {
    opacity: 1;
  }
  100% {
    opacity: 1;
  }
}

/* Separator */

hr {
  height: 14px;
  background-color: #000;
  width: 100%;
  clip-path: polygon(1px 0px, 0px 0px, 0px 0px, 8px 14px, 13px 14px, 22px 7px, 42px 6px, 49px 2px, 100% 2px, 100% 0px, 42px 0px, 35px 5px, 22px 6px, 13px 13px, 9px 13px);
  animation-name: hr;
  animation-duration: var(--glitched-long-duration);
  animation-iteration-count: infinite;
  animation-timing-function: linear; 
}

@keyframes hr {
  0% {
    transform: skew(0deg);
  }
  15% {
    transform: skew(0deg);
  }
  16% {
    transform: skew(20deg);
  }
  20% {
    transform: skew(20deg);
  }
  21% {
    transform: skew(0deg);
  }
  100% {
    right: 35px;
  }
}

/* Scrollbar */

::-webkit-scrollbar {
  background-color: var(--yellow-color);
}
::-webkit-scrollbar-button {
  display: none;
}
::-webkit-scrollbar-track {
  display: none;
}
::-webkit-scrollbar-track-piece {
  display: none;
}
::-webkit-scrollbar-thumb {
  background-color: var(--red-color);
  border-bottom: 2px solid var(--border-color);
  border-right: 2px solid var(--border-color);
  transition: background var(--glitched-duration);
}
::-webkit-scrollbar-thumb:hover {
  background-color: var(--orange-color);
}
::-webkit-scrollbar-corner {
  display: none;
}
::-webkit-resizer {
  display: none;
}
 /*  BLOCKQUOTE     */   
.pi {
  margin: 0;
  padding: 0;
  box-sizing: border-box;
}

.clearfix:after {
  visibility: hidden;
  display: block;
  font-size: 0;
  content: " ";
  clear: both;
  height: 0;
}

.clearfix {
  display: inline-block;
}

/* start commented backslash hack \*/
* html .clearfix {
  height: 1%;
}

.clearfix {
  display: block;
}

/* close commented backslash hack */
.bodyp {
  background: linear-gradient(183deg, rgba(141, 35, 46, 0.5) 1%, rgba(141, 35, 46, 0) 60%), linear-gradient(250deg, rgba(141, 35, 46, 0) 21%, rgba(141, 35, 46, 0.2) 20%, rgba(11, 35, 47, 0.2) 50%), linear-gradient(250deg, rgba(141, 35, 46, 0) 23%, rgba(141, 35, 46, 0.2) 20%, rgba(11, 35, 47, 0.2) 50%), linear-gradient(250deg, rgba(141, 35, 46, 0) 25%, rgba(141, 35, 46, 0.2) 20%, rgba(11, 35, 47, 0.2) 50%), repeating-linear-gradient(179deg, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0.1) 3px, rgba(0, 0, 0, 0.1) 3px, rgba(0, 0, 0, 0.1) 5px);
  background-color: #0b232f;
  background-repeat: no-repeat;
  background-attachment: fixed;
  padding: 30px;
  margin: 0px;
}

::-moz-selection {
  color: #ff1493;
  background: #9932cc;
}

::selection {
  color: #ff1493;
  background: #9932cc;
}
.blocktext{
  font-family:"VT32";
}
.container {
  max-width: 650px;
  margin: 0 auto;
  padding: 0 40px;
  font-family: "VT323";
  font-size: 1.3em;
  line-height: 1.3;
  color: #fff;
  text-shadow: -1px 1px 8px #ffc, 1px -1px 8px #fff;
  text-decoration: none;
  outline: 0 none;
}

.wrap-top {
  position: relative;
  margin-top: 20%;
  padding-bottom: 100px;
  height: 150px;
}
@media all and (max-width: 620px) {
  .wrap-top {
    height: 275px;
  }
}
@media all and (max-width: 350px) {
  .wrap-top {
    height: auto;
  }
}
.wrap-top #quote {
  width: 100%;
  padding-bottom: 30px;
  position: relative;
  color: #fff;
  text-shadow: -1px 1px 8px #ffc, 1px -1px ;

}
.wrap-top #quote:before {
  content: '"';
  font-size: 5em;
  position: absolute;
  left: -50px;
  top: -35px;
}

.wrap-mid {
  padding: 25px 0 100px 0;
}



.boxo {
  background-color: #000A00;
  color: #00B200;
  margin-top: 30px;
  margin-left: -30px;
  margin-right:  -30px;
}

ul.boxo li{
  list-style: square;
    font-family: "VT323", monospace;
    margin: 30px;
}
</style>
</head>

<body> 
<h1 class="cyberpunk">UNDERSTANDING AND CREATING ART WITH AI: REVIEW AND OUTLOOK</h1>

<p>February 19, 2021</p>

<p><span class="abstract"> ABSTRACT</span></p>

<p><span class="abstract">Technologies related to artificial intelligence (AI) have a strong impact on the changes of research and creative practices in visual arts. The growing number of research initiatives and creative applications that emerge in the intersection of AI and art, motivates us to examine and discuss the creative and explorative potentials of AI technologies in the context of art. This paper provides an integrated review of two facets of AI and art: 1) AI is used for art analysis and employed on digitized artwork collections; 2) AI is used for creative purposes and generating novel artworks. In the context of AI-related research for art understanding, we present a comprehensive overview of artwork datasets and recent works that address a variety of tasks such as classification, object detection, similarity retrieval, multimodal representations, computational aesthetics, etc. In relation to the role of AI in creating art, we address various practical and theoretical aspects of AI Art and consolidate related works that deal with those topics in detail. Finally, we provide a concise outlook on the future progression and potential impact of AI technologies on our understanding and creation of art. </span></p>

<p><span class="abstractkw">Keywords fine art, deep learning, computer vision, AI Art, generative art, computational creativity</span></p>

  <div class="bodyp">
  <div class="pi">
<div class="container">

  <div class="wrap-top">
    <div id="quote"><span class="blocktext"> lalalall</span></div>
    <div id="author-source" class="clearfix">
      <div id="author"></div>
      <div id="source"></div>
    </div>
  </div>
    </div>
  
    </div>
  
</div>
<div class="overflow-auto">
  <div class="boxo">
    <ul class="boxo">
<li>[1] A. RAMESH, M. PAVLOV, G. G., AND GRAY, S. Dall·e: Creating images from text., 2021.</li>
 
<li>[2] ABRY, P., WENDT, H., AND JAFFARD, S. When van gogh meets mandelbrot: Multifractal classification of painting’s texture. Signal Processing 93, 3 (2013), 554–572.</li>
 
<li>[3] ACHLIOPTAS, P., OVSJANIKOV, M., HAYDAROV, K., ELHOSEINY, M., AND GUIBAS, L. Artemis: Affective language for visual art. arXiv preprint arXiv:2101.07396 (2021).</li>
 
<li>[4] AGARWAL, S., KARNICK, H., PANT, N., AND PATEL, U. Genre and style based painting classification. In</li>
 
<li>2015 IEEE Winter Conference on Applications of Computer Vision (WACV) (2015), IEEE, pp. 588–594.</li>
 
<li>[5] ALAMEDA-PINEDA, X., RICCI, E., YAN, Y., AND SEBE, N. Recognizing emotions from abstract paintings using non-linear matrix completion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2016), pp. 5240–5248.</li>
 
<li>[6] AMIRSHAHI, S. A., HAYN-LEICHSENRING, G. U., DENZLER, J., AND REDIES, C. Jenaesthetics subjective dataset: analyzing paintings by subjective scores. In European Conference on Computer Vision (2014), Springer, pp. 3–19.</li>
 
<li>[7] BAR, Y., LEVY, N., AND WOLF, L. Classification of artistic styles using binarized features derived from a deep neural network. In Computer Vision - ECCV 2014 Workshops - Zurich, Switzerland, September 6-7 and 12, 2014, Proceedings, Part I (2014), pp. 71–84.</li>
 
<li>[8] BARALDI, L., CORNIA, M., GRANA, C., AND CUCCHIARA, R. Aligning text and document illustrations: towards visually explainable digital humanities. In 2018 24th International Conference on Pattern Recognition (ICPR) (2018), IEEE, pp. 1097–1102.</li>
 
<li>[9] BELL, P., AND IMPETT, L. Ikonographie und interaktion. computergestützte analyse von posen in bildern der heilsgeschichte. Das Mittelalter 24, 1 (2019), 31–53.</li>
 
<li>[10]        BIANCO, S., MAZZINI, D., NAPOLETANO, P., AND SCHETTINI, R. Multitask painting categorization by deep multibranch neural network. Expert Systems with Applications 135 (2019), 90–101.</li>
 
<li>[11]        BODEN, M. A. Creativity and art: Three roads to surprise. Oxford University Press, 2010.</li>
 
<li>[12]        BODEN, M. A., AND EDMONDS, E. A. What is generative art? Digital Creativity 20, 1-2 (2009), 21–46.</li>
 
<li>[13]        BONGINI, P., BECATTINI, F., BAGDANOV, A. D., AND DEL BIMBO, A. Visual question answering for cultural heritage. arXiv preprint arXiv:2003.09853 (2020).</li>
 
<li>[14]        BROCK, A., DONAHUE, J., AND SIMONYAN, K. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096 (2018).</li>
 
<li>[15]        CARNEIRO, G., DA SILVA, N. P., DEL BUE, A., AND COSTEIRA, J. P. Artistic image classification: An analysis on the printart database. In European conference on computer vision (2012), Springer, pp. 143–157.</li>
 
<li>[16]        CASTELLANO, G., LELLA, E., AND VESSIO, G. Visual link retrieval and knowledge discovery in painting datasets. Multimedia Tools and Applications (2020), 1–18.</li>
 
<li>[17]        CASTELLANO, G., AND VESSIO, G. Towards a tool for visual link retrieval and knowledge discovery in painting datasets. In Italian Research Conference on Digital Libraries (2020), Springer, pp. 105–110.</li>
 
<li>[18]        CETINIC, E. Iconographic image captioning for artworks, 2021.</li>
 
<li>[19]        CETINIC, E., AND GRGIC, S. Automated painter recognition based on image feature extraction. In 2013 55th International Symposium ELMAR (2013), IEEE, pp. 19–22.</li>
 
<li>[20]        CETINIC, E., AND GRGIC, S. Genre classification of paintings. In 2016 International Symposium ELMAR, 2016 (2016), IEEE, pp. 201–204.</li>
 
<li>[21]        CETINIC, E., LIPIC, T., AND GRGIC, S. Fine-tuning convolutional neural networks for fine art classification.</li>
 
<li>Expert Systems with Applications 114 (2018), 107–118.</li>
 
<li>[22]        CETINIC, E., LIPIC, T., AND GRGIC, S. A deep learning perspective on beauty, sentiment, and remembrance of art. IEEE Access 7 (2019), 73694–73710.</li>
 
<li>[23]        CETINIC, E., LIPIC, T., AND GRGIC, S. Learning the principles of art history with convolutional neural networks. Pattern Recognition Letters 129 (2020), 56–62.</li>
 
<li>[24]        CH’NG, E. Art by computing machinery: Is machine art acceptable in the artworld? ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) 15, 2s (2019), 1–17.</li>
 
<li>[25]        CHRISTIE’S. Is artificial intelligence set to become art’s next medium?, 2018.</li>
 
<li>[26]        COECKELBERGH, M. Can machines create art? Philosophy & Technology 30, 3 (2017), 285–303.</li>
 
<li>[27]        COLTON, S. The painting fool: Stories from building an automated painter. In Computers and creativity. Springer, 2012, pp. 3–38.</li>
 
<li>[28]        COLTON, S., PEASE, A., AND SAUNDERS, R. Issues of authenticity in autonomously creative systems. In</li>
 
<li>Proceedings of the Ninth International Conference on Computational Creativity (2018).</li>
 
<li>[29]        CROWLEY, E. J., PARKHI, O. M., AND ZISSERMAN, A. Face painting: querying art with photos.</li>
 
<li>[30]        CROWLEY, E. J., AND ZISSERMAN, A. In search of art. In European Conference on Computer Vision (2014), Springer, pp. 54–70.</li>
 
<li>[31]        CROWLEY, E. J., AND ZISSERMAN, A. The state of the art: Object retrieval in paintings using discriminative regions.</li>
 
<li>[32]        CROWLEY, E. J., AND ZISSERMAN, A. The art of detection. In European Conference on Computer Vision</li>
 
<li>(2016), Springer, pp. 721–737.</li>
 
<li>[33]        DANIELE, A., AND SONG, Y.-Z. Ai+ art= human. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (2019), pp. 155–161.</li>
 
<li>[34]        DAVID, O. E., AND NETANYAHU, N. S. Deeppainter: Painter classification using deep convolutional autoen- coders. In Artificial Neural Networks and Machine Learning - ICANN 2016 - 25th International Conference on Artificial Neural Networks, Barcelona, Spain, September 6-9, 2016, Proceedings, Part II (2016), Springer, pp. 20–28.</li>
 
<li>[35]        DENG, J., DONG, W., SOCHER, R., LI, L.-J., LI, K., AND FEI-FEI, L. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on (2009), IEEE, pp. 248–255.</li>
 
<li>[36]        DENG, Y., TANG, F., DONG, W., MA, C., HUANG, F., DEUSSEN, O., AND XU, C. Exploring the representa- tivity of art paintings. IEEE Transactions on Multimedia (2020).</li>
 
<li>[37]        DORIN, A. Chance and complexity: stochastic and generative processes in art and creativity. In Proceedings of the Virtual Reality International Conference: Laval Virtual (2013), pp. 1–8.</li>
 
<li>[38]        DORIN, A., MCCABE, J., MCCORMACK, J., MONRO, G., AND WHITELAW, M. A framework for understand- ing generative art. Digital Creativity 23, 3-4 (2012), 239–259.</li>
 
<li>[39]        EFROS, A. A., AND FREEMAN, W. T. Image quilting for texture synthesis and transfer. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques (2001), pp. 341–346.</li>
 
<li>[40]        ELGAMMAL, A. Ai is blurring the definition of artist: Advanced algorithms are using machine learning to create art autonomously. American Scientist 107, 1 (2019), 18–22.</li>
 
<li>[41]        ELGAMMAL, A., LIU, B., ELHOSEINY, M., AND MAZZONE, M. Can: Creative adversarial networks, generating" art" by learning about styles and deviating from style norms. arXiv preprint arXiv:1706.07068 (2017).</li>
 
<li>[42]        ELGAMMAL, A., LIU, B., KIM, D., ELHOSEINY, M., AND MAZZONE, M. The shape of art history in the eyes of the machine. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 (2018), AAAI press, pp. 2183–2191.</li>
 
<li>[43]        EPSTEIN, Z., LEVINE, S., RAND, D. G., AND RAHWAN, I. Who gets credit for ai-generated art?  Iscience 23, 9 (2020), 101515.</li>
 
<li>[44]        ESHRAGHIAN, J. K. Human ownership of artificial creativity. Nature Machine Intelligence 2, 3 (2020), 157–160.</li>
 
<li>[45]        FLOREA, C., CONDOROVICI, R., VERTAN, C., BUTNARU, R., FLOREA, L., AND VR NCEANU, R. Pandora: Description of a painting database for art movement recognition with baselines and perspectives. In 2016 24th European Signal Processing Conference (EUSIPCO) (2016), IEEE, pp. 918–922.</li>
 
<li>[46]        FRANCESCHET, M., COLAVIZZA, G., SMITH, T., FINUCANE, B., OSTACHOWSKI, M. L., SCALET, S., PERKINS, J., MORGAN, J., AND HERNÁNDEZ, S. Crypto art: A decentralized view. Leonardo (2020), 1–8.</li>
 
<li>[47]        GALANTER, P. What is generative art? complexity theory as a context for art theory. In In GA2003–6th Generative Art Conference (2003), Citeseer.</li>
 
<li>[48]        GARCIA, N., AND VOGIATZIS, G. How to read paintings: semantic art understanding with multi-modal retrieval. In Computer Vision - ECCV 2018 Workshops - Munich, Germany, September 8-14, 2018, Proceedings, Part II (2018), vol. 11130 of Lecture Notes in Computer Science, Springer, pp. 676–691.</li>
 
<li>[49]        GARCIA, N., YE, C., LIU, Z., HU, Q., OTANI, M., CHU, C., NAKASHIMA, Y., AND MITAMURA, T. A dataset and baselines for visual question answering on art. In European Conference on Computer Vision (2020), Springer, pp. 92–108.</li>
 
<li>[50]        GATYS, L. A., ECKER, A. S., AND BETHGE, M. Image style transfer using convolutional neural networks. In</li>
 
<li>Proceedings of the IEEE conference on computer vision and pattern recognition (2016), pp. 2414–2423.</li>
 
<li>[51]        GILLOTTE, J. L. Copyright infringement in ai-generated artworks. UC Davis L. Rev. 53 (2019), 2655.</li>
 
<li>[52]        GIRSHICK, R., DONAHUE, J., DARRELL, T., AND MALIK, J. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (2014), pp. 580–587.</li>
 
<li>[53]        GONTHIER, N., GOUSSEAU, Y., AND LADJAL, S. An analysis of the transfer learning of convolutional neural networks for artistic images. arXiv preprint arXiv:2011.02727 (2020).</li>
 
<li>[54]        GONTHIER, N., GOUSSEAU, Y., LADJAL, S., AND BONFAIT, O. Weakly supervised object detection in artworks. In Proceedings of the European Conference on Computer Vision (ECCV) (2018), pp. 0–0.</li>
 
<li>[55]        GOOCH, B., AND GOOCH, A. Non-photorealistic rendering. CRC Press, 2001.</li>
 
<li>[56]        GOODFELLOW, I., POUGET-ABADIE, J., MIRZA, M., XU, B., WARDE-FARLEY, D., OZAIR, S., COURVILLE, A., AND BENGIO, Y. Generative adversarial nets. Advances in neural information processing systems 27 (2014), 2672–2680.</li>
 
<li>[57]        GRAHAM, D. J., HUGHES, J. M., LEDER, H., AND ROCKMORE, D. N. Statistics, vision, and the analysis of artistic style. Wiley Interdisciplinary Reviews: Computational Statistics 4, 2 (2012), 115–123.</li>
 
<li>[58]        GUADAMUZ, A. Do androids dream of electric copyright? comparative analysis of originality in artificial intelligence generated works. Intellectual property quarterly (2017).</li>
 
<li>[59]        HAYN-LEICHSENRING, G. U., LEHMANN, T., AND REDIES, C. Subjective ratings of beauty and aesthetics: cor- relations with statistical image properties in western oil paintings. i-Perception 8, 3 (2017), 2041669517715474.</li>
 
<li>[60]        HERTZMANN, A. Painterly rendering with curved brush strokes of multiple sizes. In Proceedings of the 25th annual conference on Computer graphics and interactive techniques (1998), pp. 453–460.</li>
 
<li>[61]        HERTZMANN, A.  Can computers create art? In Arts (2018), vol. 7, Multidisciplinary Digital Publishing Institute, p. 18.</li>
 
<li>[62]        HERTZMANN, A. Computers do not make art, people do. Communications of the ACM 63, 5 (2020), 45–48.</li>
 
<li>[63]        HERTZMANN, A. Visual indeterminacy in gan art. Leonardo 53, 4 (2020), 424–428.</li>
 
<li>[64]        HERTZMANN, A., JACOBS, C. E., OLIVER, N., CURLESS, B., AND SALESIN, D. H. Image analogies. In</li>
 
<li>Proceedings of the 28th annual conference on Computer graphics and interactive techniques (2001), pp. 327–340.</li>
 
<li>[65]        HONG, J.-W. Bias in perception of art produced by artificial intelligence. In International Conference on Human-Computer Interaction (2018), Springer, pp. 290–303.</li>
 
<li>[66]        HONG, J.-W., AND CURRAN, N. M. Artificial intelligence, artists, and art: attitudes toward artwork produced by humans vs. artificial intelligence. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) 15, 2s (2019), 1–16.</li>
 
<li>[67]        JACOBSEN, C. R., AND NIELSEN, M. Stylometry of paintings using hidden markov modelling of contourlet transforms. Signal Processing 93, 3 (2013), 579–591.</li>
 
<li>[68]        JENICEK, T., AND CHUM, O. Linking art through human poses. In 2019 International Conference on Document Analysis and Recognition (ICDAR) (2019), IEEE, pp. 1338–1345.</li>
 
<li>[69]        JING, Y., YANG, Y., FENG, Z., YE, J., YU, Y., AND SONG, M. Neural style transfer: A review. IEEE transactions on visualization and computer graphics (2019).</li>
 
<li>[70]        KARAYEV, S., TRENTACOSTE, M., HAN, H., AGARWALA, A., DARRELL, T., HERTZMANN, A., AND WINNEMOELLER, H. Recognizing image style. In British Machine Vision Conference, BMVC 2014, Nottingham, UK, September 1-5, 2014 (2014), BMVA Press.</li>
 
<li>[71]        KARRAS, T., LAINE, S., AND AILA, T. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (2019), pp. 4401–4410.</li>
 
<li>[72]        KEREN, D. Painter identification using local features and naive bayes. In 16th International Conference on Pattern Recognition, ICPR 2002, Quebec, Canada, August 11-15, 2002. (2002), vol. 2, pp. 474–477.</li>
 
<li>[73]        KHALILI, A., AND BOUCHACHIA, H. An information theory approach to aesthetic assessment of visual patterns.</li>
 
<li>Entropy 23, 2 (2021), 153.</li>
 
<li>[74]        KHAN, F. S., BEIGPOUR, S., VAN DE WEIJER, J., AND FELSBERG, M. Painting-91: a large scale database for computational painting categorization. Machine vision and applications 25, 6 (2014), 1385–1397.</li>
 
<li>[75]        KIM, D., LIU, B., ELGAMMAL, A., AND MAZZONE, M. Finding principal semantics of style in art. In 2018 IEEE 12th International Conference on Semantic Computing (ICSC), IEEE, pp. 156–163.</li>
 
<li>[76]        KIM, D., SON, S.-W., AND JEONG, H. Large-scale quantitative analysis of painting arts. Scientific reports 4</li>
 
<li>(2014), 7370.</li>
 
<li>[77]        KIM, D., XU, J., ELGAMMAL, A., AND MAZZONE, M. Computational analysis of content in fine art paintings. In ICCC (2019), pp. 33–40.</li>
 
<li>[78]        KLINKE, H. The digital transformation of art history. In The Routledge Companion to Digital Humanities and Art History. Routledge, 2020, pp. 32–42.</li>
 
<li>[79]        LANG, S., AND OMMER, B. Attesting similarity: Supporting the organization and study of art image collections with computer vision. Digital Scholarship in the Humanities 33, 4 (2018), 845–856.</li>
 
<li>[80]        LANG, S., AND OMMER, B. Reflecting on how artworks are processed and analyzed by computer vision: Supplementary material.   In Proceedings of the European Conference on Computer Vision (ECCV) (2018), pp. 0–0.</li>
 
<li>[81]        LECOUTRE, A., NÉGREVERGNE, B., AND YGER, F. Recognizing art style automatically in painting with deep learning. In Proceedings of The 9th Asian Conference on Machine Learning, ACML 2017, Seoul, Korea, November 15-17, 2017. (2017), pp. 327–342.</li>
 
<li>[82]        LEE, B., SEO, M. K., KIM, D., SHIN, I.-S., SCHICH, M., JEONG, H., AND HAN, S. K. Dissecting landscape art history with information theory. Proceedings of the National Academy of Sciences 117, 43 (2020), 26580–26590.</li>
 
<li>[83]        LIN, H., VAN ZUIJLEN, M., WIJNTJES, M. W., PONT, S. C., AND BALA, K. Insights from a large-scale database of material depictions in paintings. arXiv preprint arXiv:2011.12276 (2020).</li>
 
<li>[84]        LLANO, M. T., D’INVERNO, M., YEE-KING, M., MCCORMACK, J., ILSAR, A., PEASE, A., AND COLTON,</li>
 
<li>S. Explainable computational creativity. In Proc. ICCC (2020).</li>
 
<li>[85]        MADHU, P., KOSTI, R., MÜHRENBERG, L., BELL, P., MAIER, A., AND CHRISTLEIN, V. Recognizing charac- ters in art history using deep learning. In Proceedings of the 1st Workshop on Structuring and Understanding of Multimedia heritAge Contents (2019), pp. 15–22.</li>
 
<li>[86]        MADHU, P., MARQUART, T., KOSTI, R., BELL, P., MAIER, A., AND CHRISTLEIN, V. Understanding compositional structures in art historical images using pose and gaze priors. In European Conference on Computer Vision (2020), Springer, pp. 109–125.</li>
 
<li>[87]        MAO, H., CHEUNG, M., AND SHE, J. Deepart: Learning joint representations of visual arts. In Proceedings of the 25th ACM international conference on Multimedia (2017), pp. 1183–1191.</li>
 
<li>[88]        MAZZONE, M., AND ELGAMMAL, A. Art, creativity, and the potential of artificial intelligence. In Arts (2019), vol. 8, Multidisciplinary Digital Publishing Institute, p. 26.</li>
 
<li>[89]        MCCORMACK, J., BOWN, O., DORIN, A., MCCABE, J., MONRO, G., AND WHITELAW, M. Ten questions concerning generative computer art. Leonardo 47, 2 (2014), 135–141.</li>
 
<li>[90]        MCCORMACK, J., GIFFORD, T., AND HUTCHINGS, P. Autonomy, authenticity, authorship and intention in computer generated art. In International Conference on Computational Intelligence in Music, Sound, Art and Design (Part of EvoStar) (2019), Springer, pp. 35–50.</li>
 
<li>[91]        MENIS-MASTROMICHALAKIS, O., SOFOU, N., AND STAMOU, G. Deep ensemble art style recognition. In</li>
 
<li>2020 International Joint Conference on Neural Networks (IJCNN) (2020), IEEE, pp. 1–8.</li>
 
<li>[92]        MENSINK, T., AND VAN GEMERT, J. The rijksmuseum challenge: Museum-centered visual recognition.  In</li>
 
<li>Proceedings of International Conference on Multimedia Retrieval (2014), pp. 451–454.</li>
 
<li>[93]        MERMET, A., KITAMOTO, A., SUZUKI, C., AND TAKAGISHI, A. Face detection on pre-modern japanese artworks using r-cnn and image patching for semi-automatic annotation. In Proceedings of the 2nd Workshop on Structuring and Understanding of Multimedia heritAge Contents (2020), pp. 23–31.</li>
 
<li>[94]        MILANI, F., AND FRATERNALI, P. A data set and a convolutional model for iconography classification in paintings. arXiv preprint arXiv:2010.11697 (2020).</li>
 
<li>[95]        MOHAMMAD, S., AND KIRITCHENKO, S. Wikiart emotions: An annotated dataset of emotions evoked by art. In Proceedings of the eleventh international conference on language resources and evaluation (LREC 2018) (2018).</li>
 
<li>[96]        MORDVINTSEV, A., OLAH, C., AND TYKA, M. Inceptionism: Going deeper into neural networks, june 2015.</li>
 
<li>URL http://googleresearch. blogspot. com/2015/06/inceptionism-going-deeper-into-neural. html (2015).</li>
 
<li>[97]        MZOUGHI, O., BIGAND, A., AND RENAUD, C. Face detection in painting using deep convolutional neural networks. In International Conference on Advanced Concepts for Intelligent Vision Systems (2018), Springer, pp. 333–341.</li>
 
<li>[98]        NOTARO, A. State-of-the-art: Ai through the (artificial) artist’s eye. In EVA London 2020: Electronic Visualisation and the Arts (2020), pp. 322–328.</li>
 
<li>[99]        POSTHUMUS, E. Brill iconclass ai test set, 2020.</li>
 
<li>[100]       QI, H., AND HUGHES, S. A new method for visual stylometry on impressionist paintings. In 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2011), IEEE, pp. 2036–2039.</li>
 
<li>[101]       RAGOT, M., MARTIN, N., AND COJEAN, S. Ai-generated vs. human artworks. a perception bias towards artificial intelligence? In Extended abstracts of the 2020 CHI conference on human factors in computing systems (2020), pp. 1–10.</li>
 
<li>[102]       REA, N. Has artificial intelligence brought us the next great art movement? here are 9 pioneering artists who are exploring ai’s creative potential., 2018.</li>
 
<li>[103]       REED, S., AKATA, Z., YAN, X., LOGESWARAN, L., SCHIELE, B., AND LEE, H. Generative adversarial text to image synthesis. In International Conference on Machine Learning (2016), PMLR, pp. 1060–1069.</li>
 
<li>[104]       RISSET, J. Stochastic processes in music and art. In Stochastic Processes in Quantum Theory and Statistical Physics. Springer, 1982, pp. 281–288.</li>
 
<li>[105]       SABATELLI, M., KESTEMONT, M., DAELEMANS, W., AND GEURTS, P. Deep transfer learning for art classification problems. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops (2018), pp. 0–0.</li>
 
<li>[106]       SANDOVAL, C., PIROGOVA, E., AND LECH, M. Two-stage deep learning approach to the classification of fine-art paintings. IEEE Access 7 (2019), 41770–41781.</li>
 
<li>[107]       SARGENTIS, G., DIMITRIADIS, P., KOUTSOYIANNIS, D., ET AL. Aesthetical issues of leonardo da vinci’s and pablo picasso’s paintings with stochastic evaluation. Heritage 3, 2 (2020), 283–305.</li>
 
<li>[108]       SEGUIN, B., STRIOLO, C., KAPLAN, F., ET AL. Visual link retrieval in a database of paintings. In European Conference on Computer Vision (2016), Springer, pp. 753–767.</li>
 
<li>[109]       SHAMIR, L., MACURA, T., ORLOV, N., ECKLEY, D. M., AND GOLDBERG, I. G. Impressionism, expression- ism, surrealism: Automated recognition of painters and schools of art. ACM Transactions on Applied Perception (TAP) 7, 2 (2010), 8.</li>
 
<li>[110]       SHAMIR, L., AND TARAKHOVSKY, J. A. Computer analysis of art. J. Comput. Cult. Herit. (JOCCH) 5, 2 (Aug. 2012), 7:1–7:11.</li>
 
<li>[111]       SHEN, X., EFROS, A. A., AND AUBRY, M. Discovering visual patterns in art collections with spatially- consistent feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2019), pp. 9278–9287.</li>
 
<li>[112]       SHENG, S., AND MOENS, M.-F. Generating captions for images of ancient artworks. In Proceedings of the 27th ACM International Conference on Multimedia (2019), pp. 2478–2486.</li>
 
<li>[113]       SIDOROVA, E. The cyber turn of the contemporary art market. In Arts (2019), vol. 8, Multidisciplinary Digital Publishing Institute, p. 84.</li>
 
<li>[114]       SIGAKI, H. Y., PERC, M., AND RIBEIRO, H. V. History of art paintings through the lens of entropy and complexity. Proceedings of the National Academy of Sciences 115, 37 (2018), E8585–E8594.</li>
 
<li>[115]       SRINIVASAN, R., AND UCHINO, K. Biases in generative art—a causal look from the lens of art history. arXiv preprint arXiv:2010.13266 (2020).</li>
 
<li>[116]       STEFANINI, M., CORNIA, M., BARALDI, L., CORSINI, M., AND CUCCHIARA, R. Artpedia: A new visual- semantic dataset with visual and contextual sentences in the artistic domain. In International Conference on Image Analysis and Processing (2019), Springer, pp. 729–740.</li>
 
<li>[117]       STEPHENSEN, J. L. Towards a philosophy of post-creative practices?–reading obvious’ "portrait of edmond de belamy". Politics of the Machine Beirut 2019 2 (2019), 21–30.</li>
 
<li>[118]       STREZOSKI, G., AND WORRING, M. Omniart: a large-scale artistic benchmark. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) 14, 4 (2018), 1–21.</li>
 
<li>[119]       TODOROV,   P. A game of dice:   Machine learning and the question concerning art.     arXiv preprint arXiv:1904.01957 (2019).</li>
 
<li>[120]       VAN NOORD, N., AND POSTMA, E. Learning scale-variant and scale-invariant features for deep image classification. Pattern Recognition 61 (2017), 583–592.</li>
 
<li>[121]       VASWANI, A., SHAZEER, N., PARMAR, N., USZKOREIT, J., JONES, L., GOMEZ, A. N., KAISER, L., AND</li>
 
<li>POLOSUKHIN, I. Attention is all you need. arXiv preprint arXiv:1706.03762 (2017).</li>
 
<li>[122]       WECHSLER, H., AND TOOR, A. S. Modern art challenges face detection. Pattern Recognition Letters 126</li>
 
<li>(2019), 3–10.</li>
 
<li>[123]       WESTLAKE, N., CAI, H., AND HALL, P. Detecting people in artwork with cnns. In European Conference on Computer Vision (2016), Springer, pp. 825–841.</li>
 
<li>[124]       WU, Y., MOU, Y., LI, Z., AND XU, K. Investigating american and chinese subjects’ explicit and implicit perceptions of ai-generated artistic work. Computers in Human Behavior 104 (2020), 106186.</li>
 
<li>[125]       XU, T., ZHANG, P., HUANG, Q., ZHANG, H., GAN, Z., HUANG, X., AND HE, X. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (2018), pp. 1316–1324.</li>
 
<li>[126]       YANG, H., AND MIN, K. Classification of basic artistic media based on a deep convolutional approach. The Visual Computer 36, 3 (2020), 559–578.</li>
 
<li>[127]       YANISKY-RAVID, S., AND VELEZ-HERNANDEZ,  L. A. Copyrightability of artworks produced by creative robots and originality: the formality-objective model. Minn. JL Sci. & Tech. 19 (2018), 1.</li>
 
<li>[128]       YANULEVSKAYA, V., UIJLINGS, J., BRUNI, E., SARTORI, A., ZAMBONI, E., BACCI, F., MELCHER, D., AND SEBE, N. In the eye of the beholder: employing statistical analysis and eye tracking for analyzing abstract paintings. In Proceedings of the 20th ACM international conference on multimedia (2012), pp. 349–358.</li>
 
<li>[129]       ZHAO, L., SHANG, M., GAO, F., LI, R., HUANG, F., AND YU, J. Representation learning of image composition for aesthetic prediction. Computer Vision and Image Understanding 199 (2020), 103024.</li>
 
<li>[130]       ZHU, J.-Y., PARK, T., ISOLA, P., AND EFROS, A. A. Unpaired image-to-image translation using cycle- consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision (2017), pp. 2223–2232.</li>
 

  </ul>
  </div>
</body>
</body>
</html>
